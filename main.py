# Objectif ‚Äî Ce script g√®re l'orchestration du pipeline (Ingestion, Recherche, G√©n√©ration) et d√©finit la classe Agent.

# √âtape 1 ‚Äî Importer les d√©pendances du syst√®me et du pipeline
import logging                                                                  # import : charger le module standard | logging : gestion des journaux d'√©v√©nements
from pathlib import Path                                                        # from : importer depuis un package | pathlib : gestion moderne des chemins | Path : classe objet chemin
from time import time                                                           # from : importer depuis le module temps | time : fonction pour mesurer la dur√©e d'ex√©cution
from typing import List                                                         # from : importer depuis le typage | typing : module types | List : type liste g√©n√©rique

# Importer toutes les classes et Singletons du projet
from src.core.config import RAW_DIR, RERANK_TOP_K                               # from : importer les constantes | src.core.config : configuration | RAW_DIR, RERANK_TOP_K : chemin du dossier brut et taille finale
from src.core.schemas import Chunk, GeneratedAnswer, SearchResult               # from : importer les sch√©mas | src.core.schemas : nos structures de donn√©es
from src.generation.llm_engine import llm_engine                                # from : importer le moteur LLM | src.generation.llm_engine : notre instance globale de Qwen (doit √™tre charg√©e)
from src.indexing.embedder import embedder                                      # from : importer l'embedder | src.indexing.embedder : notre instance FastEmbedder
from src.indexing.chunker import SemanticChunker                                # from : importer le chunker | src.indexing.chunker : outil de d√©coupage intelligent
from src.indexing.vector_store import VectorStore                               # from : importer la DB | src.indexing.vector_store : notre classe LanceDB
from src.ingestion.loader_doc import load_document                              # from : importer l'ingestion | src.ingestion.loader_doc : fonction pour PDF/DOCX
from src.ingestion.loader_web import load_url                                   # from : importer l'ingestion | src.ingestion.loader_web : fonction pour URL
from src.retrieval.cache import init_semantic_cache                             # from : importer le cache | src.retrieval.cache : fonction d'initialisation du cache
from src.retrieval.query_expansion import QueryExpander                         # from : importer l'expander | src.retrieval.query_expansion : outil HyDE
from src.retrieval.reranker import Reranker                                     # from : importer le reranker | src.retrieval.reranker : outil MXBai

# √âtape 2 ‚Äî Configurer le logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s') # logging.basicConfig(...) : configuration de base | level : niveau d'affichage | format : format du message
logger = logging.getLogger(__name__)                                            # logger : objet enregistreur

# √âtape 3 ‚Äî D√©finir la classe de l'agent VEV RAG (Le Cerveau)
class VEVAgent:                                                                 # class : d√©finir une classe | VEVAgent : l'objet principal qui orchestre le RAG

    # √âtape 3.1 ‚Äî Constructeur (Initialisation de tous les outils)
    def __init__(self):                                                         # def : constructeur | self : instance
        if llm_engine is None or embedder is None:                              # if : condition de v√©rification critique | llm_engine : moteur Qwen | or : ou | embedder : FastEmbedder
            logger.critical("Initialization failed: LLM or Embedder is missing. Check logs for details.") # logger.critical : message d'erreur fatal
            raise RuntimeError("Cannot start VEV Agent without core models.")   # raise : lever une erreur pour stopper l'ex√©cution

        logger.info("Initializing RAG components...")                           # logger.info : d√©but de l'initialisation
        self.embedder = embedder                                                # self.embedder : stocker l'embedder FastEmbedder
        self.llm = llm_engine                                                   # self.llm : stocker le moteur Qwen
        self.vector_store = VectorStore(embedder=self.embedder)                 # self.vector_store : stocker LanceDB (initialis√© avec l'embedder)
        self.chunker = SemanticChunker(embedder=self.embedder)                  # self.chunker : stocker le SemanticChunker
        self.query_expander = QueryExpander(llm_engine=self.llm)                # self.query_expander : stocker l'outil HyDE
        self.reranker = Reranker()                                              # self.reranker : stocker le Reranker MXBai
        self.cache = None                                                       # self.cache : initialis√© √† None ici, puis charg√© par app.py
        logger.info("VEV Agent core initialized.")                              # logger.info : message de succ√®s

    # √âtape 3.2 ‚Äî M√©thode du Pipeline d'Ingestion
    def ingest_document(self, path_or_url: str):                                # def : d√©finir la m√©thode | ingest_document : charge et indexe un document
        """Pipeline complet : Charger -> Nettoyer -> Chunker -> Indexer."""
        start_time = time()                                                     # start_time : enregistrer le temps de d√©but

        # 1. Chargement de la source
        if path_or_url.startswith("http"):                                      # if : si le chemin commence par "http" (c'est une URL)
            text, metadata = load_url(path_or_url)                              # text, metadata : appel √† la fonction de scraping web
        else:                                                                   # else : sinon (c'est un chemin local)
            text, metadata = load_document(path_or_url)                         # text, metadata : appel √† la fonction de chargement doc/pdf

        # 2. Chunking S√©mantique
        chunks: List[Chunk] = self.chunker.chunk_document(text, metadata)       # chunks : liste des morceaux | self.chunker.chunk_document(...) : d√©coupage intelligent
        
        if not chunks:                                                          # if : si aucun chunk n'a √©t√© cr√©√©
            logger.error("No valid chunks created after processing.")           # logger.error : message d'√©chec
            return                                                              # return : sortir de la fonction

        # 3. Indexation dans LanceDB (l'embedding est calcul√© ici)
        self.vector_store.add_chunks(chunks)                                    # self.vector_store.add_chunks(...) : ajout √† la DB (calcule les embeddings FastEmbed ici)
        
        end_time = time()                                                       # end_time : enregistrer le temps de fin
        logger.info(f"Ingestion successful ({len(chunks)} chunks). Time: {end_time - start_time:.2f}s") # logger.info : succ√®s avec la dur√©e

    # √âtape 3.3 ‚Äî M√©thode du Pipeline de Recherche (RAG)
    def ask_query(self, query: str) -> GeneratedAnswer:                         # def : d√©finir la m√©thode | ask_query : ex√©cute la recherche et la g√©n√©ration | -> : retour | GeneratedAnswer : objet r√©ponse structur√©e
        """Pipeline complet : Cache -> HyDE -> Recherche -> Rerank -> G√©n√©ration LLM."""
        start_time = time()                                                     # start_time : enregistrer le temps de d√©but

        # 1. V√©rification du Cache S√©mantique (Acc√©l√©rateur)
        if self.cache:                                                          # if : si le cache est actif (doit √™tre mis √† jour par app.py)
            cached_answer = self.cache.lookup(query)                            # cached_answer : essayer de trouver la r√©ponse avec lookup() (LanceDB)
            if cached_answer:                                                   # if : si une r√©ponse est trouv√©e
                logger.info("Cache hit! Returning cached answer.")              # logger.info : succ√®s du cache
                return GeneratedAnswer(query=query, answer=cached_answer, sources=[], processing_time=time() - start_time) # return : renvoyer la r√©ponse du cache imm√©diatement

        # 2. Transformation de Requ√™te (HyDE)
        queries_to_search = self.query_expander.expand_query(query)             # queries_to_search : requ√™te originale + document HyDE g√©n√©r√©
        
        # 3. Recherche Hybride (LanceDB)
        all_results: List[SearchResult] = []                                    # all_results : liste pour stocker tous les r√©sultats
        for q in queries_to_search:                                             # for : boucle sur chaque requ√™te (originale + HyDE)
            results = self.vector_store.search(q, top_k=RERANK_TOP_K * 2)       # results : r√©sultats de LanceDB | top_k * 2 : on prend 2x plus pour le Reranker
            all_results.extend(results)                                         # all_results.extend(...) : ajouter √† la liste principale

        # 4. Reranking (Raffinement)
        unique_results = list({r.chunk.id: r for r in all_results}.values())    # unique_results : astuce pour d√©dupliquer les chunks par leur ID
        
        final_context = self.reranker.rerank(query, unique_results)             # final_context : les 5 meilleurs documents (RERANK_TOP_K)

        if not final_context:                                                   # if : si aucun document pertinent n'a √©t√© trouv√©
            answer = "Je n'ai pas trouv√© d'information pertinente dans les documents index√©s pour r√©pondre √† cette question." # answer : message d'√©chec
            return GeneratedAnswer(query=query, answer=answer, sources=[], processing_time=time() - start_time) # return : r√©ponse simple

        # 5. Pr√©paration du Contexte LLM
        context_texts = [res.chunk.text for res in final_context]               # context_texts : extraire le texte des 5 meilleurs chunks
        context_str = "\n---\n".join(context_texts)                             # context_str : fusionner les textes avec un s√©parateur

        # 6. G√©n√©ration Finale (RAG)
        rag_prompt = (                                                          # rag_prompt : le prompt final envoy√© √† Qwen
            "Tu es VEV Agent, un assistant expert en documentation. Utilise UNIQUEMENT le contexte fourni ci-dessous pour r√©pondre √† la question.\n" # R√¥le et instruction stricte (Nouveau Nom Agent)
            "Contexte:\n"
            f"===\n{context_str}\n===\n"                                        # Contexte fusionn√©
            f"Question: {query}\n"                                              # Question de l'utilisateur
            "R√©ponse d√©taill√©e:"                                                # Instruction de d√©but de r√©ponse
        )

        final_answer = self.llm.generate(prompt=rag_prompt)                     # final_answer : appel au moteur Qwen
        
        # 7. Mise en Cache de la r√©ponse
        if self.cache:                                                          # if : si le cache est actif
            self.cache.store(query, final_answer)                               # self.cache.store(...) : enregistrer la question/r√©ponse (LanceDB)

        # 8. Renvoyer la r√©ponse structur√©e
        end_time = time()                                                       # temps final
        return GeneratedAnswer(                                                 # return : objet r√©ponse complet
            query=query,
            answer=final_answer,
            sources=final_context,
            processing_time=end_time - start_time
        )

# √âtape 4 ‚Äî Fonction run_cli() (Pour le d√©bogage)
def run_cli():                                                                  # def : d√©finir la fonction CLI | run_cli : boucle de ligne de commande
    # Ce code n'est plus le point d'entr√©e principal. Il est l√† uniquement pour permettre un test rapide sans Streamlit.
    logger.warning("Main script running in CLI mode for debug purposes.")       # logger.warning : avertissement √† l'utilisateur
    try:                                                                        # try : tenter d'initialiser
        agent = VEVAgent()                                                      # agent : instance de l'agent
    except RuntimeError:                                                        # except : si l'initialisation √©choue
        return                                                                  # return : arr√™ter la fonction

    while True:                                                                 # while True : boucle infinie
        user_input = input("\n[VEV]> ")                                         # user_input : demander l'entr√©e utilisateur
        if user_input.lower() in ['quit', 'exit']:                              # if : si l'utilisateur veut quitter
            break                                                               # break : sortir de la boucle

        if user_input.lower().startswith("ingest "):                            # if : si l'utilisateur veut indexer
            source = user_input.split(" ", 1)[1].strip()                        # source : extraire le chemin/URL apr√®s "ingest "
            if source:                                                          # if : si la source est non vide
                agent.ingest_document(source)                                   # agent.ingest_document(...) : lancer le pipeline d'ingestion
            continue                                                            # continue : revenir au d√©but de la boucle

        if user_input.strip():                                                  # if : si c'est une question de recherche
            try:                                                                # try : tenter de r√©pondre
                response = agent.ask_query(user_input)                          # response : appel au pipeline RAG
                print("\nü§ñ R√©ponse VEV Agent:")                                # print : afficher le titre r√©ponse
                print(response.answer)                                          # print : afficher la r√©ponse g√©n√©r√©e
                print(f"\n[Temps: {response.processing_time:.2f}s | Sources utilis√©es ({len(response.sources)}):]") # print : afficher les m√©triques
                for src in response.sources:                                    # for : boucle sur les sources
                    print(f"  - (Score {src.score:.4f}) {src.chunk.metadata.title} (Page {src.chunk.metadata.page_number})") # print : afficher les d√©tails de la source
            except Exception as e:                                              # except : si une erreur arrive pendant le chat
                logger.error(f"Error during query processing: {e}")             # logger.error : loguer l'erreur
                print("Une erreur est survenue lors du traitement de la requ√™te.") # print : message utilisateur

# √âtape 5 ‚Äî Ex√©cuter le CLI si le script est lanc√© directement
if __name__ == "__main__":                                                      # if : condition python standard (script principal)
    run_cli()                                                                   # run_cli() : lancer la boucle de ligne de commande